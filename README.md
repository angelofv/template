# Template â€“Â MLÂ PipelineÂ Starter

[![CI](https://github.com/angelofv/template/actions/workflows/ci.yml/badge.svg)](https://github.com/angelofv/template/actions/workflows/ci.yml)
[![CD](https://github.com/angelofv/template/actions/workflows/cd.yml/badge.svg)](https://github.com/angelofv/template/actions/workflows/cd.yml)
[![codecov](https://codecov.io/gh/angelofv/template/graph/badge.svg?token=RD0GRZMER0)](https://codecov.io/gh/angelofv/template)

> **A readyâ€‘toâ€‘use scaffold for building, tracking and deploying small machineâ€‘learning projects.**
>
> MixÂ &Â match **PrefectÂ 3** flows, **MLflow** tracking, **Kedro** dataâ€‘catalogs, **FastAPI** microâ€‘services and **Streamlit** demos â€“ all shipped in reproducible **Docker** images and guarded by GitHubÂ Actions **CIÂ â†”Â CD**.

---

## âœ¨Â Highlights

| Capability       | Whatâ€™s inside                   | Why care?                                                            |
| ---------------- | ------------------------------- | -------------------------------------------------------------------- |
| Orchestration    | PrefectÂ 3 `@flow` tasks         | Dependency graph, logs, retries & scheduling **without extra infra** |
| Experiment track | MLflow fileâ€‘store (local)       | Compare runs; rich UI exposed at portÂ `5000`                         |
| Data layer       | Kedro `DataCatalog`             | Declarative datasets (CSV, Parquet, PickleÂ â€¦)                        |
| Serving          | FastAPIÂ (+â€¯uvicorn) & Streamlit | From REST inference to an interactive playground                     |
| Dev ergonomics   | Makefile, Ruff, Conda, Docker   | Oneâ€‘liners & consistent environments                                 |
| Quality gates    | pytestâ€‘cov, Ruff, Codecovâ€¯badge | Keep techâ€‘debt under control                                         |
| CIâ€¯/â€¯CD          | GHâ€¯ActionsÂ â‡†Â GHCR + Trivy       | Pushâ€‘toâ€‘image pipeline with security scans                           |

---

> **Headsâ€‘up ğŸ–¼ï¸**
> The Streamlit dashboard is only a starting point.
> Make it yours by editing `services/app.py` **or** by exporting environment variables such as `PROFILE_NAME`, `PROFILE_DESC`, `PROFILE_AVATAR`, `LINK_GITHUB`, etc.

---

## âš¡Â Quick start

### 0.Â Prerequisites

* **PythonÂ 3.10+**
* **DockerÂ Desktop** (or Podman/Colima)
* (optional) **Conda**Â â‰¥â€¯4.10
* **Codecov** â€“ create a free account, enable your repo and add `CODECOV_TOKEN` as a secret in **Settingsâ€¯â†’â€¯Secretsâ€¯â†’â€¯Actions**.

### 1â€‘A.Â Native workflow (no Docker)

```bash
make create_env          # conda env â€˜templateâ€™ (oneâ€‘shot)
conda activate template
make requirements        # pip install

make local-infra         # spinâ€‘up MLflowÂ :5000 + PrefectÂ :4200
make local-pipeline      # run full flow
make local-serve         # start APIÂ :8000 + StreamlitÂ :8501
# â€¦ hack, commit, profit! â€¦
make local-down          # stop all local services
```

### 1â€‘B.Â Dockerâ€‘first workflow

```bash
make infra      # spinâ€‘up MLflow + Prefect in containers
make pipeline   # build pipeline image & run full flow
make serve      # build & start API + Streamlit

# Tearâ€‘down
make down
```

<details>
<summary>TL;DR</summary>

```bash
docker compose up --build
```

`docker compose` will launch everything, but you will lose the pretty, colourâ€‘coded logs provided by the MakefileÂ ğŸ™ƒ.

</details>

---

## ğŸ› ï¸Â Everyâ€‘day commands

| Command                                                        | Purpose                                   |
| -------------------------------------------------------------- | ----------------------------------------- |
| `make format`                                                  | Autoâ€‘format the whole codeâ€‘base with Ruff |
| `make lint`                                                    | Static analysis                           |
| `make test`                                                    | pytest + coverage                         |
| `make clean`                                                   | Remove Python artefacts & caches          |
| `make mlflow-clean`                                            | Wipe local `mlruns/` folder               |
| `make infra / pipeline / serve / down`                         | **Docker** helpers                        |
| `make local-infra / local-pipeline / local-serve / local-down` | **Native** helpers                        |

Run `make help` to see every available target and its description.

---

## ğŸ§ªÂ Running tests locally

Tests require the repo root on **PYTHONPATH** so that `import src` resolves.

```bash
make test                # recommended â€“ sets env var automatically
```

If you must, run manually:

```bash
export PYTHONPATH="$PWD:$PYTHONPATH"
pytest -q
```

> `ModuleNotFoundError: src`? Check the current working directory or `$PYTHONPATH`.

---

## ğŸ“¦Â CIÂ /Â CD pipeline (GitHubÂ Actions)

* **ci.yml** â€“ on every push / PR
  1.Â Setâ€‘up Python & cache pip
  2.Â `make lint` & `make test`
  3.Â Upload coverage to Codecov
* **cd.yml** â€“ after successful CI
  1.Â Build multiâ€‘stage image; push to **GHCR** (`:latest`â€¯+â€¯SHA)
  2.Â Scan with **Trivy** (fail on critical/high vulns)
  3.Â Run smoke test (`import src`) inside the container

Badges at the top of this file always reflect the last run.

---

## ğŸ—‚Â Project layout

```
â”œâ”€â”€ src/                  # Prefect flows, model, plotting
â”‚   â”œâ”€â”€ extract.py        # load_raw_data & preprocess tasks
â”‚   â”œâ”€â”€ train.py          # train_model task
â”‚   â”œâ”€â”€ plot_metrics.py   # plot_metrics task
â”‚   â””â”€â”€ run.py            # orchestrates the flow
â”œâ”€â”€ services/             # FastAPI (api.py) & Streamlit (app.py)
â”œâ”€â”€ configs/              # config.yaml + Kedro catalog
â”œâ”€â”€ data/                 # 01_raw / 02_processed / 03_models / 04_reports
â”œâ”€â”€ notebooks/            # Exploratory notebooks (ignored by Docker)
â”œâ”€â”€ Dockerfile            # Multiâ€‘stage build (pipeline / api / app)
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ Makefile              # Dev helpers
â””â”€â”€ tests/                # tiny unit + smoke suite
```

---

## âš™ï¸Â Configuration

All pipeline tunables live in a single YAML file:

| File                  | Consumed by                            | Notes               |
| --------------------- | -------------------------------------- | ------------------- |
| `configs/config.yaml` | every Prefect task via `load_config()` | See `src/config.py` |

Override the path by exporting `CONFIG_PATH`.

The Kedro **DataCatalog** is defined in `configs/catalog.yaml`.

---

## ğŸŒÂ Environment variables

Tasks read their configuration from **environment variables** first, then fall back to defaults. Create a `.env` file or export them in your shell.

| Variable              | Default                     | Component          | Purpose                              |
| --------------------- | --------------------------- | ------------------ | ------------------------------------ |
| `MLFLOW_TRACKING_URI` | `file:./mlruns`             | pipeline, API      | Where MLflow stores runs & artefacts |
| `MLFLOW_EXPERIMENT`   | `Default`                   | pipeline           | MLflow experiment name               |
| `PREFECT_API_URL`     | `http://localhost:4200/api` | pipeline           | Prefect REST endpoint                |
| `MODEL_PATH`          | `data/03_models/model.pkl`  | FastAPI, Streamlit | Pickled model used for inference     |
| `API_URL`             | `http://localhost:8000`     | Streamlit          | Endpoint for prediction requests     |
| `PROFILE_*`, `LINK_*` | â€“                           | Streamlit          | Dashboard personalisation            |

---

## ğŸ—ï¸Â Extending

1. **Change the model** â€“ edit `train_model()` and add hyperâ€‘params to `config.yaml`.
2. **Add new tasks** â€“ write a Prefect `@flow` function and wire it in `src/run.py`.
3. **Ship notebooks** â€“ mount them inside the image or copy in `Dockerfile` when needed.
4. **Deploy** â€“ pull `ghcr.io/<user>/template:<tag>` on any container platform.

---

## ğŸ“œÂ License

Released under the **MIT License** â€“ see [`LICENSE`](LICENSE) for details.
